<div align="center">
<!-- <img width="1200" height="475" alt="GHBanner" src="https://github.com/user-attachments/assets/0aa67016-6eaf-458a-adb2-6e31a0763ed6" /> -->
</div>

# Journal App

Submission-ready repository for the Journal App assignment.

This project implements a chat-style journaling application with support for:

- Adding journal entries (notes, reminders, shopping list items) via natural language prompts.
- Querying the shopping list using natural questions ("What is my shopping list?", "What should I buy?").
- A simple hallucination guard: out-of-scope requests (e.g. math/general knowledge) receive a polite refusal.
- An in-memory server-side store so the app runs without an LLM key by default. The code also includes places to integrate a real LLM (Vercel Generative UI SDK / Gemini / OpenAI) behind a server endpoint.

This README explains how to run the app, test the assignment flows, and how to enable full LLM integration if you have an API key.

---

## Quick demo (what to try)

Open the app and try these messages (copy-paste):

- "Remind me to buy eggs next time I'm at the supermarket"
- "Add milk to my shopping list"
- "Alice says 'I should check out Kritunga for their awesome biryani'"
- "What is my shopping list?"
- "I'm at the supermarket. What should I buy?"
- "What is 2+2?" (should reply with a refusal: "I'm only a journaling app…")

## How to run locally

Prerequisites: Node.js (v18+ recommended) and npm.

1. Install dependencies

```cmd
npm install
```

2. Run the dev server

```cmd
npm run dev
```

3. Open your browser

http://localhost:3000/

4. Build for production

```cmd
npm run build
```

The `dist/` directory will contain the production build.

## Implementation notes

- UI: React + Vite; components are in `components/`.
- In-memory journaling logic: `services/vercelService.ts`. This file contains a small rule-based parser to:
  - Detect add intents and create entries.
  - Detect shopping list queries and return items.
  - Guard against simple out-of-scope requests (math, general knowledge).
- There is also `services/geminiService.ts` present (an earlier GenAI integration sketch). For a fully model-driven implementation, integrate one of the SDKs and route requests through a secure server endpoint.

## How to enable real LLM integration (optional)

If you want the assistant responses generated by a model using function-calling semantics, you can wire a server-side integration. Recommended approach:

1. Create a server endpoint (e.g., `/api/chat`) that receives user messages from the client.
2. On the server, call the model (Vercel Generative SDK, Gemini, or OpenAI) with a system instruction and function declarations (addJournalEntry, getJournalEntries).
3. If the model emits a function call, execute it against the in-memory store, append the result to the conversation, and ask the model for a final response.

Environment variable suggestions (examples):

- `GENAI_API_KEY` or `OPENAI_API_KEY` — store the provider key on the server only; do NOT commit it.

I can implement the server endpoint and model integration if you provide which provider/key you prefer.

## Files relevant to the assignment

- `App.tsx` — main UI wiring
- `components/` — `ChatInput.tsx`, `ChatMessage.tsx`, `ShoppingListView.tsx`
- `services/vercelService.ts` — in-memory journaling + heuristics
- `services/geminiService.ts` — GenAI-based implementation sketch (kept as reference)
- `types.ts` and `constants.ts` — types and function declarations

## Known limitations

- Persistence: current storage is in-memory (module-scoped). Data is lost when the server restarts. For persistent storage, integrate a database or localStorage for the client.
- Heuristics: `vercelService` uses simple regex and string checks to detect intents/keywords; a real LLM + function calling will be more robust.
- No automated tests included (I can add a Playwright-based E2E smoke test on request).

## Submission notes

I included a `SUBMISSION.md` file with a suggested reply-all email template and checklist for reviewers.

---

If you'd like, I can now:

1. Integrate the Vercel Generative UI SDK (or OpenAI) behind a server endpoint and demonstrate true function-calling (requires an API key).
2. Add an automated smoke test (Playwright) that verifies the add/query flows end-to-end.
3. Create a submission ZIP containing the repo minus `node_modules` (I can produce the exact PowerShell command to create it).

Tell me which of the above you want me to do next and I will implement it.
